= Neural Search
// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

Search comprises of performing four primary steps:

* generate a representation of the query that specifies the information need
* generate a representation of the document that captures the information contained
* match the query and the document representations from the corpus of information
* assign a score to each matched document in order to establish a meaningful document ranking by relevance in the results

With the *Neural Search* module Apache Solr is introducing support for neural networks based techniques that can improve various aspects of search.

They can be differentiated based on whether they affect the query representation, the document representation, or the estimation of the relevance score.

Neural Search is an industry derivation from the academic field of https://www.microsoft.com/en-us/research/uploads/prod/2017/06/fntir2018-neuralir-mitra.pdf[Neural information Retrieval].

== Neural Search Concepts

=== Deep Learning

More and more frequently, we hear about how Artificial Intelligence (AI) permeates every aspect of our lives.

When we talk about AI we are referring to a superset of techniques that enable machines to learn and show intelligence like humans.

Since computing power has strongly and steadily advanced in the recent past, AI has seen a resurgence lately and it is now used in many domains, including software engineering and Information Retrieval (the science that regulates Search Engines and similar systems).

In particular the advent of https://en.wikipedia.org/wiki/Deep_learning[Deep Learning] introduced the use of deep neural networks to solve complex problems that could not simply be solved by an algorithm.

=== Dense Vector Representation 


=== Dense Retrieval

== Index Time
This is the list of Apache Solr field types designed to support Neural Search:

=== DenseVectorField
The Dense Vector field gives the possibility of indexing and searching dense vectors of float elements.

e.g.

`[1.0, 2.5, 3.7, 4.1]` (a comma separated list of float elements, starting and ending with a square bracket)

Here's how `DenseVectorField` should be configured in the schema:

[source,xml]
<fieldType name="knn_vector" class="solr.DenseVectorField" vectorDimension="4" similarityFunction="cosine"/>
<field name="vector" type="knn_vector" indexed="true" stored="true"/>

`vectorDimension`::
+
[%autowidth,frame=none]
|===
|Mandatory
|===
+
The dimension of the dense vector to pass in.
Accepted values:
Integer < = 1024.

`similarityFunction`::
+
[%autowidth,frame=none]
|===
|Optional |Default: `euclidean`
|===
+
Vector similarity function; used in search to return top K most similar vectors to a target vector.
Accepted values: `euclidean`, `dot_product`  or `cosine`.

* `euclidean`: https://en.wikipedia.org/wiki/Euclidean_distance[Euclidean distance]
* `dot_product`: https://en.wikipedia.org/wiki/Dot_product[Dot product]. NOTE: this similarity is intended as an optimized way to perform cosine similarity. In order to use it, all vectors must be of unit length, including both document and query vectors. Using dot product with vectors that are not unit length can result in errors or poor search results..
* `cosine`: https://en.wikipedia.org/wiki/Cosine_similarity[Cosine similarity]. NOTE: the preferred way to perform cosine similarity is to normalize all vectors to unit length, and instead use DOT_PRODUCT. You should only use this function if you need to preserve the original vectors and cannot normalize them in advance.


DenseVectorField supports enabling the attributes: `indexed`, `stored`.

N.B. currently multivalue is not supported

Here's how a `DenseVectorField` should be indexed:

[.dynamic-tabs]
--
[example.tab-pane#json]
====
[.tab-label]*JSON*
[source,json]
----
[{ "id": "1",
"vector": "[1.0, 2.5, 3.7, 4.1]"
},
{ "id": "2",
"vector": "[1.5, 5.5, 6.7, 65.1]"
}
]
----
====

[example.tab-pane#xml]
====
[.tab-label]*XML*
[source,xml]
----
<add>
<doc>
<field name="id">1</field>
<field name="vector">[1.0, 2.5, 3.7, 4.1]</field>
</doc>
<doc>
<field name="id">2</field>
<field name="vector">[1.5, 5.5, 6.7, 65.1]</field>
</doc>
</add>
----
====

[example.tab-pane#solrj]
====
[.tab-label]*SolrJ*
[source,java,indent=0]
----
final SolrClient client = getSolrClient();

final SolrInputDocument d1 = new SolrInputDocument();
d1.setField("id", "1");
d1.setField("vector", "[1.0, 2.5, 3.7, 4.1]");

final SolrInputDocument d2 = new SolrInputDocument();
d2.setField("id", "2");
d2.setField("vector", "[1.5, 5.5, 6.7, 65.1]");

client.add(Arrays.asList(d1, d2));
----
====
--

== Query Time
This is the list of Apache Solr query approaches designed to support Neural Search:

=== knn Query Parser
The `knn` K Nearest Neighbors query parser allows to find the k nearest documents to the target vector according to indexed dense vectors in the given field.

It takes the following parameters:

`f`::
+
[%autowidth,frame=none]
|===
|Mandatory
|===
+
The DenseVectorField to search in.

`topK`::
+
[%autowidth,frame=none]
|===
|Optional |Default: 10
|===
+
How many nearest k results to return.

Here's how to run a KNN search:

[source,text]
&q={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]

The search results retrieved are the nearest K to the vector in input `[1.0, 2.0, 3.0, 4.0]`, ranked by the similarityFunction configured at indexing time.

==== Usage with Filter Queries
The `knn` query parser can be used in filter queries:
[source,text]
&q=id:(1 2 3)&fq={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]

The `knn` query parser can be used with filter queries:
[source,text]
&q={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]&fq=id:(1 2 3)

[IMPORTANT]
====
When using `knn` in these scenarios make sure you have clear how filter queries work in Apache Solr:

The Ranked List of document IDs resulting from the main query `q` is intersected with the set of document IDs deriving from each filter query `fq`.

e.g.

Ranked List from `q`=`[ID1, ID4, ID2, ID10]` <intersects> Set from `fq`=`{ID3, ID2, ID9, ID4}` = `[ID4,ID2]`
====


==== Usage as Re-Ranking Query
The `knn` query parser can be used to rerank first pass query results:
[source,text]
&q=id:(3 4 9 2)&rq={!rerank reRankQuery=$rqq reRankDocs=4 reRankWeight=1}&rqq={!knn f=vector topK=10}[1.0, 2.0, 3.0, 4.0]

[IMPORTANT]
====
When using `knn` in reranking pay attention to the `topK` parameter.

The second pass score(deriving from knn) is calculated only if the documend `d` from the first pass is within
the K nearest neighbors(*in the whole index*) of the target vector to search.

This means the second pass `knn` is executed on the whole index anyway, which is a current limitation.

The final ranked list of results will have the first pass score(main query `q`) combined with the second pass score(the approximated similarityFunction distance to the target vector to search).
====